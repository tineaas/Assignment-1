{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8babf911",
   "metadata": {},
   "source": [
    "# 1 Load Data Set & Data Preprocessing\n",
    "In this section the three data sets \"website-phising.cvs\", \"bcp.csv\" and \"arrhythmia.csv\" will be loaded using Pandas. When the data sets are loaded, there will also be some preprocessing before implementation of the classifing methods. ? will be replaces with NA, so that these values can be replaced by the mode of the column. The mode of the class is the majority value in the column. To make sure the categorical values can be used, they will be converted into numerical representation. This is due to errors in implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef6b8e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website Phishing: \n",
      "   having_IP_Address   URL_Length   Shortining_Service   having_At_Symbol  \\\n",
      "1                  1            1                    1                  1   \n",
      "2                  1            0                    1                  1   \n",
      "3                  1            0                    1                  1   \n",
      "4                  1            0                   -1                  1   \n",
      "5                 -1            0                   -1                  1   \n",
      "\n",
      "    double_slash_redirecting   Prefix_Suffix   having_Sub_Domain  \\\n",
      "1                          1              -1                   0   \n",
      "2                          1              -1                  -1   \n",
      "3                          1              -1                  -1   \n",
      "4                          1              -1                   1   \n",
      "5                         -1              -1                   1   \n",
      "\n",
      "    SSLfinal_State   Domain_registeration_length   Favicon  ...  \\\n",
      "1                1                            -1         1  ...   \n",
      "2               -1                            -1         1  ...   \n",
      "3               -1                             1         1  ...   \n",
      "4                1                            -1         1  ...   \n",
      "5                1                            -1         1  ...   \n",
      "\n",
      "    popUpWidnow    Iframe   age_of_domain    DNSRecord     web_traffic   \\\n",
      "1              1        1               -1            -1              0   \n",
      "2              1        1                1            -1              1   \n",
      "3              1        1               -1            -1              1   \n",
      "4             -1        1               -1            -1              0   \n",
      "5              1        1                1             1              1   \n",
      "\n",
      "    Page_Rank   Google_Index   Links_pointing_to_page   Statistical_report  \\\n",
      "1          -1              1                        1                    1   \n",
      "2          -1              1                        0                   -1   \n",
      "3          -1              1                       -1                    1   \n",
      "4          -1              1                        1                    1   \n",
      "5          -1              1                       -1                   -1   \n",
      "\n",
      "     Class   \n",
      "1        -1  \n",
      "2        -1  \n",
      "3        -1  \n",
      "4         1  \n",
      "5         1  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "BCP: \n",
      "   Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
      "1             1002945                5                        4   \n",
      "2             1015425                3                        1   \n",
      "3             1016277                6                        8   \n",
      "4             1017023                4                        1   \n",
      "5             1017122                8                       10   \n",
      "\n",
      "   Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
      "1                         4                  5                            7   \n",
      "2                         1                  1                            2   \n",
      "3                         8                  1                            3   \n",
      "4                         1                  3                            2   \n",
      "5                        10                  8                            7   \n",
      "\n",
      "   Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
      "1           10                3                2        1      2  \n",
      "2            2                3                1        1      2  \n",
      "3            4                3                7        1      2  \n",
      "4            1                3                1        1      2  \n",
      "5           10                9                7        1      4  \n",
      "\n",
      "Arrhythmia: \n",
      "   age   sex   height   weight   QRSduration   PRinterval   Q-Tinterval  \\\n",
      "1   56     1      165       64            81          174           401   \n",
      "2   54     0      172       95           138          163           386   \n",
      "3   55     0      175       94           100          202           380   \n",
      "4   75     0      190       80            88          181           360   \n",
      "5   13     0      169       51           100          167           321   \n",
      "\n",
      "    Tinterval   Pinterval   QRS  ... chV6_QwaveAmp chV6_RwaveAmp  \\\n",
      "1         149          39    25  ...           0.0           8.5   \n",
      "2         185         102    96  ...           0.0           9.5   \n",
      "3         179         143    28  ...           0.0          12.2   \n",
      "4         177         103   -16  ...           0.0          13.1   \n",
      "5         174          91   107  ...          -0.6          12.2   \n",
      "\n",
      "  chV6_SwaveAmp chV6_RPwaveAmp chV6_SPwaveAmp  chV6_PwaveAmp  chV6_TwaveAmp  \\\n",
      "1           0.0            0.0            0.0            0.2            2.1   \n",
      "2          -2.4            0.0            0.0            0.3            3.4   \n",
      "3          -2.2            0.0            0.0            0.4            2.6   \n",
      "4          -3.6            0.0            0.0           -0.1            3.9   \n",
      "5          -2.8            0.0            0.0            0.9            2.2   \n",
      "\n",
      "   chV6_QRSA  chV6_QRSTA  class  \n",
      "1       20.4        38.8      6  \n",
      "2       12.3        49.0     10  \n",
      "3       34.6        61.6      1  \n",
      "4       25.4        62.8      7  \n",
      "5       13.5        31.1     14  \n",
      "\n",
      "[5 rows x 280 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#Preprocessing of files\n",
    "def read_file(filename):\n",
    "    df = pd.read_csv(filename) #Read file\n",
    "    df.replace('?', pd.NA, inplace=True) #Replace ? with NA\n",
    "    df = df.drop(df.index[0]) #Drop header row\n",
    "    replace_missing_values(df) #Replace the missing values\n",
    "    numericalConverter(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Convert categorical columns to numerical representation\n",
    "def numericalConverter(df):\n",
    "    df = df.astype(float)\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df[col] = label_encoder.fit_transform(df[col]) \n",
    "\n",
    "#Replace missing values with mode\n",
    "def replace_missing_values(df):\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "wp = read_file('website-phishing.csv')\n",
    "bcp = read_file('bcp.csv')\n",
    "ar = read_file('arrhythmia.csv')\n",
    "\n",
    "#Printing preview of data sets\n",
    "print(\"Website Phishing: \")\n",
    "print(wp.head())\n",
    "print(\"\\nBCP: \")\n",
    "print(bcp.head())\n",
    "print(\"\\nArrhythmia: \")\n",
    "print(ar.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8221f8d",
   "metadata": {},
   "source": [
    "# 2 Implement Decision Trees\n",
    "In this section the decision stump-, unpruned decision tree-, and pruned decision tree-methods will be implemented. After that the methods will be applied to the data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf2452",
   "metadata": {},
   "source": [
    "## 2.1 Implementations of methods\n",
    "### 2.1.1 Decision Stump\n",
    "A decision stump is a decision tree with only one split. Since there is only one split, the possible splits will be evaluated based on accuracy, since that is what the model it self will be evaluated by. A problem that might arise is that we can see from section 1 that the data sets have more than to classes, and the method that will bli implemented only splits the data into two subsets. Therefore, it is expected to observe a low accuracy when implementing this model in section 2.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ed0663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.feature_index = None #Split feature\n",
    "        self.threshold = None #Split value\n",
    "        self.classes = None #class labels\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y) #get class labels\n",
    "        best_accuracy = 0\n",
    "\n",
    "        # Iterate over each feature and threshold to find the best split\n",
    "        for feature_index in range(X.shape[1]): #iterate over all the features\n",
    "            thresholds = np.unique(X[:, feature_index]) #find the corresponding threshold to the feature\n",
    "            for threshold in thresholds: #iterate over all the thresholds\n",
    "\n",
    "                #Predict all samples in X based on feature and threshold\n",
    "                predictions = np.where(X[:, feature_index] <= threshold, self.classes[0], self.classes[1]) #OBS: Only predicts one of two first classes\n",
    "\n",
    "                #Accuracy of this prediction\n",
    "                accuracy = np.mean(y == predictions)\n",
    "\n",
    "                # Update the best split if accuracy improves\n",
    "                if accuracy > best_accuracy:\n",
    "                    self.feature_index = feature_index\n",
    "                    self.threshold = threshold\n",
    "                    best_accuracy = accuracy\n",
    "\n",
    "    #Predict class for sample x. OBS: Only predicts one of two first classes\n",
    "    def predict_instance(self, x):\n",
    "        if float(x[self.feature_index]) <= float(self.threshold):\n",
    "            return self.classes[0]\n",
    "        else:\n",
    "            return self.classes[1]\n",
    "\n",
    "    def predict(self, X):\n",
    "        #Make sure X is a DataFrame\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        predictions = [] #empty list of predictions\n",
    "        for _, x in X.iterrows():\n",
    "            predictions.append(self.predict_instance(x)) #add predictions\n",
    "        return np.array(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40e5c5",
   "metadata": {},
   "source": [
    "### 2.1.2 Decision Tree\n",
    "To evaluate a split in a decision tree where the depth is higher than 1, it is better to evaluate a split based on information gain, rather than just accuracy. This is because a split can sort the nodes, and therefore provide information to the tree, without the accuracy being improved in that spesific split. The following decision tree will use entropy and information gain to evaluate what the best split is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d19fca",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "To compare the impurity before and after a split, I need a function for calculating entropy. Simply explained, entropy is a way of measuring disorder, and is used to measure the randomness in a set. The formula for entropy is: \n",
    "$$\n",
    "E(S) = \\sum_{i=1}^{c} - p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Where S is the startset, and $p_i$ is the propability of value i. To avoid logarithm of zero, $1e-9$ is added to $p_i$. This addition is minor, but will make avoid numeric instability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8b8bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    p_i = y.value_counts()/y.shape[0]\n",
    "    e = np.sum(-p_i * np.log2(p_i + 1e-9))\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8692c",
   "metadata": {},
   "source": [
    "#### Information gain\n",
    "To evaluate the splits, I will calculate the entropy before and anfter the split. The difference will give me a measure of the information gain of the split. If the entropy is lower (less randomness in the subsets) after the split, the information gain will be positive. I want to choose the split that provides the highest information gain. The formula for information gain is: \n",
    "$$\n",
    "InformationGain= Entropy(S) - \\sum_{k} \\frac{|k|}{|S|}Entropy(k)\n",
    "$$\n",
    "\n",
    "Where S is the start set, and $k_i$ is the i-th subset. The fraction factor is weighting the entropy of each of the subsets based on the number of instances in the subset, compared with the total number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1994925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y, left, right):\n",
    "    y_entropy = entropy(y)\n",
    "    left_entropy = entropy(left)\n",
    "    right_entropy = entropy(right)\n",
    "    ig = y_entropy - (len(left)/len(y)*left_entropy+len(right)/len(y)*right_entropy)\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480562be",
   "metadata": {},
   "source": [
    "#### Functions to find Best Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92ef1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return left and right node. OBS: Only binary split\n",
    "def split(X, y, feature, threshold):\n",
    "    bol = X.iloc[:,feature] <= threshold\n",
    "    left = y[bol]\n",
    "    right = y[~bol]\n",
    "    return left, right\n",
    "\n",
    "#Find the best split based on information gain\n",
    "def get_best_split(X, y):\n",
    "    best_ig = -1\n",
    "    best_threshold = None\n",
    "    best_feature_index = None\n",
    "\n",
    "    for feature_id in range (1, X.shape[1]):\n",
    "        thresholds = np.unique(X.iloc[: ,feature_id].tolist())\n",
    "        for threshold_value in thresholds:\n",
    "            left, right = split(X,y,feature_id, threshold_value)\n",
    "            ig = information_gain(y,left,right)\n",
    "            if ig > best_ig:\n",
    "                best_ig, best_threshold,best_feature_index = ig, threshold_value, feature_id\n",
    "    return best_feature_index, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fcb636",
   "metadata": {},
   "source": [
    "#### Decision Tree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ab2136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:    \n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "\n",
    "    def grow_tree(self, X, y, depth=0):\n",
    "            \n",
    "            #Stopping criteria\n",
    "            if len(np.unique(y)) == 1:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            \n",
    "            best_feature_index, best_threshold = get_best_split(X, y)\n",
    "            if best_feature_index is None:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            bol = X.iloc[:, best_feature_index] <= best_threshold\n",
    "            y_left = y[bol]\n",
    "            y_right = y[~bol]\n",
    "            if y_left.empty or y_right.empty:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            left_tree = self.grow_tree(X[bol], y_left, depth + 1)\n",
    "            right_tree = self.grow_tree(X[~bol], y_right, depth + 1)\n",
    "            return (best_feature_index, best_threshold, left_tree, right_tree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        predictions = []\n",
    "        for _, x in X.iterrows():\n",
    "            predictions.append(self.pred_instance(x, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def pred_instance(self, x, tree):\n",
    "        if isinstance(tree, np.int64):\n",
    "            return tree  # Return the predicted class directly\n",
    "        feature_index, threshold, left_branch, right_branch = tree\n",
    "        if float(x[feature_index]) <= float(threshold):\n",
    "            return self.pred_instance(x, left_branch)\n",
    "        else:\n",
    "            return self.pred_instance(x, right_branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b8b4f",
   "metadata": {},
   "source": [
    "### 2.2.1 Pruned Tree\n",
    "To prevent the decision tree form overfitting on the training data, a pruning method can be implemented. Pruning methods can be divided into post- and pre-pruning. Post-pruning is pruning of the tree after it is fully trained. This means that the tree is first overfitted, and then simplified to correct for the overfitting. This can take a lot of computational capacity if the tree is vert deep and the data set is big. Pre-pruning methods is aiming to stop growing the tree before it is too overfitt. In this implementation there will be a pre-pruning method where max_depth will be used. Max_depth ensures that the tree does not grow deeper than wanted. Max_depth is a hyperparameter that will be further explored in section 3. I have chosen to implement a pre-pruning method to save computational complexity, and therefore saving time and resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d58c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrunedTree:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "    \n",
    "    def grow_tree(self, X, y, depth=0):\n",
    "            #Stopping criteria. Stop if leaf node, or max_depth is reached\n",
    "            if len(np.unique(y)) == 1 or depth == self.max_depth:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            \n",
    "            #Get the split with the highest information gain\n",
    "            best_feature_index, best_threshold = get_best_split(X, y)\n",
    "\n",
    "            #If no split provides information gain -> leaf node\n",
    "            if best_feature_index is None:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            \n",
    "            #Split the date into left and right node based on best feature and corresponding threshold\n",
    "            bol = X.iloc[:, best_feature_index] <= best_threshold\n",
    "            y_left = y[bol]\n",
    "            y_right = y[~bol]\n",
    "\n",
    "            #If all samples are in one node -> leaf node\n",
    "            if y_left.empty or y_right.empty:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            \n",
    "            #If not, grow three further\n",
    "            left_tree = self.grow_tree(X[bol], y_left, depth + 1)\n",
    "            right_tree = self.grow_tree(X[~bol], y_right, depth + 1)\n",
    "\n",
    "            #Return splitting feature, threshold, left and right node\n",
    "            return (best_feature_index, best_threshold, left_tree, right_tree)\n",
    "\n",
    "    #The tree grows differently than \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.grow_tree(X, y)\n",
    "\n",
    "    #Predict class label of x\n",
    "    def pred_instance(self, x, tree):\n",
    "        if isinstance(tree, np.int64):\n",
    "            return tree  # Return the predicted class directly\n",
    "        feature_index, threshold, left_branch, right_branch = tree\n",
    "        if float(x[feature_index]) <= float(threshold):\n",
    "            return self.pred_instance(x, left_branch)\n",
    "        else:\n",
    "            return self.pred_instance(x, right_branch)\n",
    "        \n",
    "    #Predict class labels for matrix X\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        predictions = []\n",
    "        for _, x in X.iterrows():\n",
    "            predictions.append(self.pred_instance(x, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ec6f8",
   "metadata": {},
   "source": [
    "### 2.2 Apply Methods to Data Sets\n",
    "In this section the three methods will be applied to the three data sets loaded and preprocessed in section 1. To apply the methods we have to split the data sets into feature and target, as well as splitting them into training and test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c60ba",
   "metadata": {},
   "source": [
    "#### Train & Test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e46add9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WP:  (8843, 30) (2211, 30)\n",
      "BCP:  (545, 10) (137, 10)\n",
      "AR:  (360, 279) (91, 279)\n"
     ]
    }
   ],
   "source": [
    "#Last column is target label\n",
    "def feature_target_split(df):\n",
    "    X = df.iloc[:, 1:] \n",
    "    y = df.iloc[:, 0]\n",
    "    return X, y\n",
    "\n",
    "#Divide data set\n",
    "X1, y1 = feature_target_split(wp)\n",
    "X2, y2 = feature_target_split(bcp)\n",
    "X3, y3 = feature_target_split(ar)\n",
    "\n",
    "\n",
    "RANDOM_STATE = 1506\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(\"WP: \", X1_train.shape,X1_test.shape)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(\"BCP: \", X2_train.shape,X2_test.shape)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(\"AR: \", X3_train.shape,X3_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e536f",
   "metadata": {},
   "source": [
    "#### Decision Stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14a79d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Stump Accuracy\n",
      "Webiste Phising: 0.736318407960199\n",
      "BCP:  0.0\n",
      "Arrhythmia: 0.01098901098901099\n"
     ]
    }
   ],
   "source": [
    "wp_stump = DecisionStump()\n",
    "bcp_stump = DecisionStump()\n",
    "ar_stump = DecisionStump()\n",
    "\n",
    "def trainStump(tree, X_train, y_train, X_test, y_test):\n",
    "    tree.fit(X_train.values, y_train)\n",
    "    y_pred = tree.predict(X_test.values)\n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    return acc\n",
    "\n",
    "wp_stump_accuracy = trainStump(wp_stump, X1_train, y1_train, X1_test, y1_test)\n",
    "bcp_stump_accuracy = trainStump(bcp_stump, X2_train, y2_train, X2_test, y2_test)\n",
    "ar_stump_accuracy = trainStump(ar_stump, X3_train, y3_train, X3_test, y3_test)\n",
    "\n",
    "print(\"Decision Stump Accuracy\")\n",
    "print(\"Webiste Phising:\", wp_stump_accuracy)\n",
    "print(\"BCP: \", bcp_stump_accuracy)\n",
    "print(\"Arrhythmia:\", ar_stump_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06981cc",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4e39a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy\n",
      "Webiste Phising: 0.8516508367254636\n",
      "BCP:  0.0072992700729927005\n",
      "Arrhythmia: 0.01098901098901099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#Webiste-phising\\ntree1 = DecisionTree()   \\ntree1.fit(X1_train, y1_train)\\ny1_pred = tree1.predict(X1_test)\\nacc1_tree = np.mean(y1_test == y1_pred)\\nprint(\"Accuracy webiste-phising:\", acc1_tree)\\n\\n#BCP\\ntree2 = DecisionTree()\\ntree2.fit(X2_train, y2_train)\\ny2_pred = tree2.predict(X2_test)\\nacc2_tree = np.mean(y2_test == y2_pred)\\nprint(\"Accuracy bcp:\", acc2_tree)\\n\\n#Arrhythmia\\ntree3 = DecisionTree()\\ntree3.fit(X3_train, y3_train)\\ny3_pred = tree3.predict(X3_test)\\nacc3_tree = np.mean(y3_test == y3_pred)\\nprint(\"Accuracy: arrhythmia\", acc3_tree)\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tree = DecisionTree()\n",
    "bcp_tree = DecisionTree()\n",
    "ar_tree = DecisionTree()\n",
    "\n",
    "def trainTree(tree, X_train, y_train, X_test, y_test):\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    return acc\n",
    "\n",
    "print(\"Decision Tree Accuracy\")\n",
    "wp_tree_accuracy = trainTree(wp_tree, X1_train, y1_train, X1_test, y1_test)\n",
    "print(\"Webiste Phising:\", wp_tree_accuracy)\n",
    "\n",
    "bcp_tree_accuracy = trainTree(bcp_tree, X2_train, y2_train, X2_test, y2_test)\n",
    "print(\"BCP: \", bcp_tree_accuracy)\n",
    "\n",
    "ar_tree_accuracy = trainTree(ar_tree, X3_train, y3_train, X3_test, y3_test)\n",
    "print(\"Arrhythmia:\", ar_tree_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ab36e",
   "metadata": {},
   "source": [
    "#### Pruned Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b576c818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy\n",
      "Webiste Phising: 0.8398914518317503\n",
      "BCP:  0.0\n",
      "Arrhythmia: 0.0\n"
     ]
    }
   ],
   "source": [
    "wp_pruned = PrunedTree(5) #max depth = 5\n",
    "bcp_pruned = PrunedTree(5) #max depth = 5\n",
    "ar_pruned = PrunedTree(5) #max depth = 5\n",
    "\n",
    "def trainTree(tree, X_train, y_train, X_test, y_test):\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    return acc\n",
    "\n",
    "print(\"Decision Tree Accuracy\")\n",
    "wp_pruned_accuracy = trainTree(wp_pruned, X1_train, y1_train, X1_test, y1_test)\n",
    "print(\"Webiste Phising:\", wp_pruned_accuracy)\n",
    "\n",
    "bcp_pruned_accuracy = trainTree(bcp_pruned, X2_train, y2_train, X2_test, y2_test)\n",
    "print(\"BCP: \", bcp_pruned_accuracy)\n",
    "\n",
    "ar_pruned_accuracy = trainTree(ar_pruned, X3_train, y3_train, X3_test, y3_test)\n",
    "print(\"Arrhythmia:\", ar_pruned_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d446a11",
   "metadata": {},
   "source": [
    "# 3 Choosing hyperparameter\n",
    "The only hyperparameters I have used in these implementations is max_depth. I will not use a 5-fold cross-validation to evaluate the max_depth values 2, 4, 5 and 10. Evaluating the different values for the hyperparameter without the cross-validation had a running time of 39m 6.5s, and implementing a 5-fold cross-validation will make the program more complex, and therefore add time. The code gets more complex becaise the tree is trained 5 times, instead of 1. If I were to implement the cross validation i would split the training data into five subsets, where one of the subset (20% of the train data) is used as i local test set. All the five subsets would be used as a test set, and the average of the accuracies would be the representative accuracy for the hyperparameter. When doing this, we try to correct for the possibility of a unnatural good split in the tree, due to luck. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "43e9e2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Max Depth  Accuracy (Website-Phising)  Running time (Website-Phising)  \\\n",
      "0          1                    0.736318                        0.704417   \n",
      "1          2                    0.739484                        1.194588   \n",
      "2          5                    0.839891                        9.036296   \n",
      "3         10                    0.844867                       59.562936   \n",
      "\n",
      "   Accuracy (BCP)  Running time (BCP)  Accuracy (Arrhythmia)  \\\n",
      "0        0.007299            0.462351               0.021978   \n",
      "1        0.000000            1.363699               0.021978   \n",
      "2        0.000000            6.762355               0.000000   \n",
      "3        0.007299           35.671880               0.010989   \n",
      "\n",
      "   Running time (Arrhythmia)  \n",
      "0                  68.676146  \n",
      "1                 160.059243  \n",
      "2                 576.857196  \n",
      "3                1426.188093  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "max_depth_values = [2, 4, 5, 10]\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, max_depth):\n",
    "    start_time = time.time()\n",
    "    model = PrunedTree(max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = np.mean(y_test == y_pred)\n",
    "    running_time = time.time() - start_time\n",
    "    return accuracy, running_time\n",
    "\n",
    "max_depth_values = [1, 2, 5, 10]\n",
    "\n",
    "results = []\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    acc1, time1 = evaluate_model(X1_train, y1_train, X1_test, y1_test, max_depth)\n",
    "    acc2, time2 = evaluate_model(X2_train, y2_train, X2_test, y2_test, max_depth)\n",
    "    acc3, time3 = evaluate_model(X3_train, y3_train, X3_test, y3_test, max_depth)\n",
    "\n",
    "    results.append([max_depth, acc1, time1, acc2, time2, acc3, time3])\n",
    "\n",
    "columns = [\"Max Depth\", \"Accuracy (Website-Phising)\", \"Running time (Website-Phising)\",\n",
    "           \"Accuracy (BCP)\", \"Running time (BCP)\",\n",
    "           \"Accuracy (Arrhythmia)\", \"Running time (Arrhythmia)\"]\n",
    "\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147c925",
   "metadata": {},
   "source": [
    "To evaluate the best value for max_depth I will look at accuracy and running time. \n",
    "\n",
    "###### Website Phising\n",
    "From the table above we can see that for Website Phising, the accuracy is improved with over 10% when going from depth 2 to 5, while the running time is only increased by ca 8 sec. For a depth of 10 we can see that there is a slight increase in accuracy, about 0.5%, and an increase in running time at about 50 sec. Based on this, I choose the max_depth = 5 for the Website phising data set.\n",
    "\n",
    "###### BCP\n",
    "For the BCP data set we can see that the accuracy of a max_depth of 1 and 10 is the same, while the depth of 2 and 5 both have a 0% accuracy. This might point in the direction that something about the implementation of the pruned tree is wrong. As we know, the BCP data set har multiple classes, and the pruned tree only classifies samples in one of the two first classes. Therefore it looks like the trees with max_depth= 1 and 10 might be lucky when choosing the two classes. Here it would be better to implement a tree for multi-class labels, and then include a max_depth to this. However, with the tree that has been implemented, I would choose the max_depth = 1.\n",
    "\n",
    "###### Arrhythmia\n",
    "Here we can also see that there might be a mistake in the implementation of the decision tree. Like explained previously, there should be a tree that classifies the samples in more than two classes, since the data set has multiple class labels. We can see that the best accuracy and running time is max_depth = 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5fbec",
   "metadata": {},
   "source": [
    "# 4 Comparing models\n",
    "In this section I will compare the three methods and comment if any of the methods preforms significantly worse on each data set. To do this, I will run a p-value test. I will also update the accuracy scores of the pruned trees with the chosen hyperparameters from section 3. As we can se, only the BCP and Arrhythmia data sets have new values for the max_depth parameter.\n",
    "\n",
    "##### T-test\n",
    "For this t-test the H0 is that the model is not significantly worse then the other. In a t-test we se if there is a reason to throw away the H0 hypothesis. The alternate hypothesis, H1, is that the model is significantly worse. We throw away H0 if the p-value is less than 0.05. The p-values is the significance level, and tells us how likely it is to observe what we have observed, assuming H0 is  true. As we can see, none of the models are significantly worse. However, this might be due to the low accuracy in  the BCP and Arrhythmia data set, which is because the decision tree only does binary splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec3177b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-test for Decision stump & Decision Tree:\n",
      "t-statistic: -1.0440295848510426\n",
      "p-value: 0.40607210639704483\n",
      "Not significantly worse\n",
      " \n",
      "T-test for Decision stump & Pruned Tree:\n",
      "t-statistic: -1.2897704922530193\n",
      "p-value: 0.3261490123224887\n",
      "Not significantly worse\n",
      " \n",
      "T-test for Decision Tree & Pruned Tree:\n",
      "t-statistic: -0.12724667374462778\n",
      "p-value: 0.910385037869117\n",
      "Not significantly worse\n",
      " \n",
      "T-test for Decision Tree & Decision stump:\n",
      "t-statistic: 1.0440295848510426\n",
      "p-value: 0.40607210639704483\n",
      "Not significantly worse\n",
      " \n",
      "T-test for Decision Tree & Pruned Tree:\n",
      "t-statistic: -0.12724667374462778\n",
      "p-value: 0.910385037869117\n",
      "Not significantly worse\n",
      " \n",
      "T-test for Pruned Tree & Decision stump:\n",
      "t-statistic: 1.2897704922530193\n",
      "p-value: 0.3261490123224887\n",
      "Not significantly worse\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "bcp_pruned_accuracy = df.loc[df['Max Depth'] == 1, 'Accuracy (BCP)'].values[0]\n",
    "ar_pruned_accuracy = df.loc[df['Max Depth'] == 1, 'Accuracy (Arrhythmia)'].values[0]\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have accuracy scores for each model\n",
    "accuracy_scores = {\n",
    "    'Decision stump': [wp_stump_accuracy, bcp_stump_accuracy, ar_stump_accuracy],\n",
    "    'Decision Tree': [wp_tree_accuracy, bcp_tree_accuracy, bcp_tree_accuracy],\n",
    "    'Pruned Tree': [wp_pruned_accuracy, bcp_pruned_accuracy, ar_pruned_accuracy]\n",
    "}\n",
    "\n",
    "# Perform t-test for each pair of models\n",
    "for model1, model2 in [('Decision stump', 'Decision Tree'), ('Decision stump', 'Pruned Tree'), ('Decision Tree', 'Pruned Tree'), ('Decision Tree', 'Decision stump'), ('Decision Tree', 'Pruned Tree'), ('Pruned Tree', 'Decision stump') ]:\n",
    "    t_statistic, p_value = stats.ttest_rel(accuracy_scores[model1], accuracy_scores[model2])\n",
    "    print(f\"T-test for {model1} & {model2}:\")\n",
    "    print(f\"t-statistic: {t_statistic}\")\n",
    "    print(f\"p-value: {p_value}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Significantly worse\")\n",
    "        print(\" \")\n",
    "    else:\n",
    "        print(\"Not significantly worse\")\n",
    "        print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db8fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
