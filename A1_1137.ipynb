{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8babf911",
   "metadata": {},
   "source": [
    "# 1 Load Data Set & Data Preprocessing\n",
    "In this section the three data sets \"website-phising.cvs\", \"bcp.csv\" and \"arrhythmia.csv\" will be loaded using Pandas. When the data sets are loaded, there will also be some preprocessing before implementation of the classifing methods. ? will be replaces with NA, so that these values can be replaced by the mode of the column. The mode of the class is the majority value in the column. To make sure the categorical values can be used, they will be converted into numerical representation. This is due to errors in implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef6b8e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website Phishing: \n",
      "   having_IP_Address   URL_Length   Shortining_Service   having_At_Symbol  \\\n",
      "1                  1            1                    1                  1   \n",
      "2                  1            0                    1                  1   \n",
      "3                  1            0                    1                  1   \n",
      "4                  1            0                   -1                  1   \n",
      "5                 -1            0                   -1                  1   \n",
      "\n",
      "    double_slash_redirecting   Prefix_Suffix   having_Sub_Domain  \\\n",
      "1                          1              -1                   0   \n",
      "2                          1              -1                  -1   \n",
      "3                          1              -1                  -1   \n",
      "4                          1              -1                   1   \n",
      "5                         -1              -1                   1   \n",
      "\n",
      "    SSLfinal_State   Domain_registeration_length   Favicon  ...  \\\n",
      "1                1                            -1         1  ...   \n",
      "2               -1                            -1         1  ...   \n",
      "3               -1                             1         1  ...   \n",
      "4                1                            -1         1  ...   \n",
      "5                1                            -1         1  ...   \n",
      "\n",
      "    popUpWidnow    Iframe   age_of_domain    DNSRecord     web_traffic   \\\n",
      "1              1        1               -1            -1              0   \n",
      "2              1        1                1            -1              1   \n",
      "3              1        1               -1            -1              1   \n",
      "4             -1        1               -1            -1              0   \n",
      "5              1        1                1             1              1   \n",
      "\n",
      "    Page_Rank   Google_Index   Links_pointing_to_page   Statistical_report  \\\n",
      "1          -1              1                        1                    1   \n",
      "2          -1              1                        0                   -1   \n",
      "3          -1              1                       -1                    1   \n",
      "4          -1              1                        1                    1   \n",
      "5          -1              1                       -1                   -1   \n",
      "\n",
      "     Class   \n",
      "1        -1  \n",
      "2        -1  \n",
      "3        -1  \n",
      "4         1  \n",
      "5         1  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "BCP: \n",
      "   Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
      "1             1002945                5                        4   \n",
      "2             1015425                3                        1   \n",
      "3             1016277                6                        8   \n",
      "4             1017023                4                        1   \n",
      "5             1017122                8                       10   \n",
      "\n",
      "   Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
      "1                         4                  5                            7   \n",
      "2                         1                  1                            2   \n",
      "3                         8                  1                            3   \n",
      "4                         1                  3                            2   \n",
      "5                        10                  8                            7   \n",
      "\n",
      "   Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
      "1           10                3                2        1      2  \n",
      "2            2                3                1        1      2  \n",
      "3            4                3                7        1      2  \n",
      "4            1                3                1        1      2  \n",
      "5           10                9                7        1      4  \n",
      "\n",
      "Arrhythmia: \n",
      "   age   sex   height   weight   QRSduration   PRinterval   Q-Tinterval  \\\n",
      "1   56     1      165       64            81          174           401   \n",
      "2   54     0      172       95           138          163           386   \n",
      "3   55     0      175       94           100          202           380   \n",
      "4   75     0      190       80            88          181           360   \n",
      "5   13     0      169       51           100          167           321   \n",
      "\n",
      "    Tinterval   Pinterval   QRS  ... chV6_QwaveAmp chV6_RwaveAmp  \\\n",
      "1         149          39    25  ...           0.0           8.5   \n",
      "2         185         102    96  ...           0.0           9.5   \n",
      "3         179         143    28  ...           0.0          12.2   \n",
      "4         177         103   -16  ...           0.0          13.1   \n",
      "5         174          91   107  ...          -0.6          12.2   \n",
      "\n",
      "  chV6_SwaveAmp chV6_RPwaveAmp chV6_SPwaveAmp  chV6_PwaveAmp  chV6_TwaveAmp  \\\n",
      "1           0.0            0.0            0.0            0.2            2.1   \n",
      "2          -2.4            0.0            0.0            0.3            3.4   \n",
      "3          -2.2            0.0            0.0            0.4            2.6   \n",
      "4          -3.6            0.0            0.0           -0.1            3.9   \n",
      "5          -2.8            0.0            0.0            0.9            2.2   \n",
      "\n",
      "   chV6_QRSA  chV6_QRSTA  class  \n",
      "1       20.4        38.8      6  \n",
      "2       12.3        49.0     10  \n",
      "3       34.6        61.6      1  \n",
      "4       25.4        62.8      7  \n",
      "5       13.5        31.1     14  \n",
      "\n",
      "[5 rows x 280 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#Preprocessing of files\n",
    "def read_file(filename):\n",
    "    df = pd.read_csv(filename) #Read file\n",
    "    df.replace('?', pd.NA, inplace=True) #Replace ? with NA\n",
    "    df = df.drop(df.index[0]) #Drop header row\n",
    "    replace_missing_values(df) #Replace the missing values\n",
    "    numericalConverter(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Convert categorical columns to numerical representation\n",
    "def numericalConverter(df):\n",
    "    df = df.astype(float)\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df[col] = label_encoder.fit_transform(df[col]) \n",
    "\n",
    "#Replace missing values with mode\n",
    "def replace_missing_values(df):\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "wp = read_file('website-phishing.csv')\n",
    "bcp = read_file('bcp.csv')\n",
    "ar = read_file('arrhythmia.csv')\n",
    "\n",
    "#Printing preview of data sets\n",
    "print(\"Website Phishing: \")\n",
    "print(wp.head())\n",
    "print(\"\\nBCP: \")\n",
    "print(bcp.head())\n",
    "print(\"\\nArrhythmia: \")\n",
    "print(ar.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8221f8d",
   "metadata": {},
   "source": [
    "# 2 Implement Decision Trees\n",
    "In this section the decision stump-, unpruned decision tree-, and pruned decision tree-methods will be implemented. After that the methods will be applied to the data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf2452",
   "metadata": {},
   "source": [
    "## 2.1 Implementations of methods\n",
    "### 2.1.1 Decision Stump\n",
    "A decision stump is a decision tree with only one split. Since there is only one split, the possible splits will be evaluated based on accuracy, since that is what the model it self will be evaluated by. A problem that might arise is that we can see from section 1 that the data sets have more than to classes, and the method that will bli implemented only splits the data into two subsets. Therefore, it is expected to observe a low accuracy when implementing this model in section 2.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ed0663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.feature_index = None #Split feature\n",
    "        self.threshold = None #Split value\n",
    "        self.classes = None #class labels\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y) #get class labels\n",
    "        best_accuracy = 0\n",
    "\n",
    "        # Iterate over each feature and threshold to find the best split\n",
    "        for feature_index in range(X.shape[1]): #iterate over all the features\n",
    "            thresholds = np.unique(X[:, feature_index]) #find the corresponding threshold to the feature\n",
    "            for threshold in thresholds: #iterate over all the thresholds\n",
    "\n",
    "                #Predict all samples in X based on feature and threshold\n",
    "                predictions = np.where(X[:, feature_index] <= threshold, self.classes[0], self.classes[1]) #OBS: Only predicts one of two first classes\n",
    "\n",
    "                #Accuracy of this prediction\n",
    "                accuracy = np.mean(y == predictions)\n",
    "\n",
    "                # Update the best split if accuracy improves\n",
    "                if accuracy > best_accuracy:\n",
    "                    self.feature_index = feature_index\n",
    "                    self.threshold = threshold\n",
    "                    best_accuracy = accuracy\n",
    "\n",
    "    #Predict class for sample x. OBS: Only predicts one of two first classes\n",
    "    def predict_instance(self, x):\n",
    "        if float(x[self.feature_index]) <= float(self.threshold):\n",
    "            return self.classes[0]\n",
    "        else:\n",
    "            return self.classes[1]\n",
    "\n",
    "    def predict(self, X):\n",
    "        #Make sure X is a DataFrame\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        predictions = [] #empty list of predictions\n",
    "        for _, x in X.iterrows():\n",
    "            predictions.append(self.predict_instance(x)) #add predictions\n",
    "        return np.array(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40e5c5",
   "metadata": {},
   "source": [
    "### 2.1.2 Decision Tree\n",
    "To evaluate a split in a decision tree where the depth is higher than 1, it is better to evaluate a split based on information gain, rather than just accuracy. This is because a split can sort the nodes, and therefore provide information to the tree, without the accuracy being improved in that spesific split. The following decision tree will use entropy and information gain to evaluate what the best split is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d19fca",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "To compare the impurity before and after a split, I need a function for calculating entropy. Simply explained, entropy is a way of measuring disorder, and is used to measure the randomness in a set. The formula for entropy is: \n",
    "$$\n",
    "E(S) = \\sum_{i=1}^{c} - p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Where S is the startset, and $p_i$ is the propability of value i. To avoid logarithm of zero, $1e-9$ is added to $p_i$. This addition is minor, but will make avoid numeric instability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8b8bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    p_i = y.value_counts()/y.shape[0]\n",
    "    e = np.sum(-p_i * np.log2(p_i + 1e-9))\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8692c",
   "metadata": {},
   "source": [
    "#### Information gain\n",
    "To evaluate the splits, I will calculate the entropy before and anfter the split. The difference will give me a measure of the information gain of the split. If the entropy is lower (less randomness in the subsets) after the split, the information gain will be positive. I want to choose the split that provides the highest information gain. The formula for information gain is: \n",
    "$$\n",
    "InformationGain= Entropy(S) - \\sum_{k} \\frac{|k|}{|S|}Entropy(k)\n",
    "$$\n",
    "\n",
    "Where S is the start set, and $k_i$ is the i-th subset. The fraction factor is weighting the entropy of each of the subsets based on the number of instances in the subset, compared with the total number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1994925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y, left, right):\n",
    "    y_entropy = entropy(y)\n",
    "    left_entropy = entropy(left)\n",
    "    right_entropy = entropy(right)\n",
    "    ig = y_entropy - (len(left)/len(y)*left_entropy+len(right)/len(y)*right_entropy)\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480562be",
   "metadata": {},
   "source": [
    "#### Functions to find Best Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92ef1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return left and right node. OBS: Only binary split\n",
    "def split(X, y, feature, threshold):\n",
    "    bol = X.iloc[:,feature] <= threshold\n",
    "    left = y[bol]\n",
    "    right = y[~bol]\n",
    "    return left, right\n",
    "\n",
    "#Find the best split based on information gain\n",
    "def get_best_split(X, y):\n",
    "    best_ig = -1\n",
    "    best_threshold = None\n",
    "    best_feature_index = None\n",
    "\n",
    "    for feature_id in range (1, X.shape[1]):\n",
    "        thresholds = np.unique(X.iloc[: ,feature_id].tolist())\n",
    "        for threshold_value in thresholds:\n",
    "            left, right = split(X,y,feature_id, threshold_value)\n",
    "            ig = information_gain(y,left,right)\n",
    "            if ig > best_ig:\n",
    "                best_ig, best_threshold,best_feature_index = ig, threshold_value, feature_id\n",
    "    return best_feature_index, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fcb636",
   "metadata": {},
   "source": [
    "#### Decision Tree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ab2136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:    \n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "\n",
    "    def grow_tree(self, X, y, depth=0):\n",
    "            \n",
    "            #Stopping criteria\n",
    "            if len(np.unique(y)) == 1:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            \n",
    "            best_feature_index, best_threshold = get_best_split(X, y)\n",
    "            if best_feature_index is None:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            bol = X.iloc[:, best_feature_index] <= best_threshold\n",
    "            y_left = y[bol]\n",
    "            y_right = y[~bol]\n",
    "            if y_left.empty or y_right.empty:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            left_tree = self.grow_tree(X[bol], y_left, depth + 1)\n",
    "            right_tree = self.grow_tree(X[~bol], y_right, depth + 1)\n",
    "            return (best_feature_index, best_threshold, left_tree, right_tree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        predictions = []\n",
    "        for _, x in X.iterrows():\n",
    "            predictions.append(self.pred_instance(x, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def pred_instance(self, x, tree):\n",
    "        if isinstance(tree, np.int64):\n",
    "            return tree  # Return the predicted class directly\n",
    "        feature_index, threshold, left_branch, right_branch = tree\n",
    "        if float(x[feature_index]) <= float(threshold):\n",
    "            return self.pred_instance(x, left_branch)\n",
    "        else:\n",
    "            return self.pred_instance(x, right_branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b8b4f",
   "metadata": {},
   "source": [
    "### 2.2.1 Pruned Tree\n",
    "To prevent the decision tree form overfitting on the training data, a pruning method can be implemented. Pruning methods can be divided into post- and pre-pruning. Post-pruning is pruning of the tree after it is fully trained. This means that the tree is first overfitted, and then simplified to correct for the overfitting. This can take a lot of computational capacity if the tree is vert deep and the data set is big. Pre-pruning methods is aiming to stop growing the tree before it is too overfitt. In this implementation there will be a pre-pruning method where max_depth will be used. Max_depth ensures that the tree does not grow deeper than wanted. Max_depth is a hyperparameter that will be further explored in section 3. I have chosen to implement a pre-pruning method to save computational complexity, and therefore saving time and resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d58c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrunedTree:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "    \n",
    "    def grow_tree(self, X, y, max_depth, depth=0):\n",
    "            #Stopping criteria. Stop if leaf node, or max_depth is reached\n",
    "            if len(np.unique(y)) == 1 or depth == max_depth:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            \n",
    "            #Get the split with the highest information gain\n",
    "            best_feature_index, best_threshold = get_best_split(X, y)\n",
    "\n",
    "            #If no split provides information gain -> leaf node\n",
    "            if best_feature_index is None:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            \n",
    "            #Split the date into left and right node based on best feature and corresponding threshold\n",
    "            bol = X.iloc[:, best_feature_index] <= best_threshold\n",
    "            y_left = y[bol]\n",
    "            y_right = y[~bol]\n",
    "\n",
    "            #If all samples are in one node -> leaf node\n",
    "            if y_left.empty or y_right.empty:\n",
    "                return y.mode()[0]  # Majority class in leaf node\n",
    "            \n",
    "            #If not, grow three further\n",
    "            left_tree = self.grow_tree(X[bol], y_left, depth + 1)\n",
    "            right_tree = self.grow_tree(X[~bol], y_right, depth + 1)\n",
    "\n",
    "            #Return splitting feature, threshold, left and right node\n",
    "            return (best_feature_index, best_threshold, left_tree, right_tree)\n",
    "\n",
    "    #The tree grows differently than \n",
    "    def fit(self, X, y, max_depth):\n",
    "        self.tree = self.grow_tree(X, y, max_depth)\n",
    "\n",
    "    #Predict class label of x\n",
    "    def pred_instance(self, x, tree):\n",
    "        if isinstance(tree, np.int64):\n",
    "            return tree  # Return the predicted class directly\n",
    "        feature_index, threshold, left_branch, right_branch = tree\n",
    "        if float(x[feature_index]) <= float(threshold):\n",
    "            return self.pred_instance(x, left_branch)\n",
    "        else:\n",
    "            return self.pred_instance(x, right_branch)\n",
    "        \n",
    "    #Predict class labels for matrix X\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        predictions = []\n",
    "        for _, x in X.iterrows():\n",
    "            predictions.append(self.pred_instance(x, self.tree))\n",
    "        return np.array(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ec6f8",
   "metadata": {},
   "source": [
    "### 2.2 Apply Methods to Data Sets\n",
    "In this section the three methods will be applied to the three data sets loaded and preprocessed in section 1. To apply the methods we have to split the data sets into feature and target, as well as splitting them into training and test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c60ba",
   "metadata": {},
   "source": [
    "#### Train & Test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e46add9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WP:  (8843, 30) (2211, 30)\n",
      "BCP:  (545, 10) (137, 10)\n",
      "AR:  (360, 279) (91, 279)\n"
     ]
    }
   ],
   "source": [
    "#Last column is target label\n",
    "def feature_target_split(df):\n",
    "    X = df.iloc[:, 1:] \n",
    "    y = df.iloc[:, 0]\n",
    "    return X, y\n",
    "\n",
    "#Divide data set\n",
    "X1, y1 = feature_target_split(wp)\n",
    "X2, y2 = feature_target_split(bcp)\n",
    "X3, y3 = feature_target_split(ar)\n",
    "\n",
    "\n",
    "RANDOM_STATE = 1506\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(\"WP: \", X1_train.shape,X1_test.shape)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(\"BCP: \", X2_train.shape,X2_test.shape)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(\"AR: \", X3_train.shape,X3_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e536f",
   "metadata": {},
   "source": [
    "#### Decision Stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14a79d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Stump Accuracy\n",
      "Webiste Phising: 0.736318407960199\n",
      "BCP:  0.0\n",
      "Arrhythmia: 0.01098901098901099\n"
     ]
    }
   ],
   "source": [
    "wp_stump = DecisionStump()\n",
    "bcp_stump = DecisionStump()\n",
    "ar_stump = DecisionStump()\n",
    "\n",
    "def trainStump(tree, X_train, y_train, X_test, y_test):\n",
    "    tree.fit(X_train.values, y_train)\n",
    "    y_pred = tree.predict(X_test.values)\n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    return acc\n",
    "\n",
    "wp_stump_accuracy = trainStump(wp_stump, X1_train, y1_train, X1_test, y1_test)\n",
    "bcp_stump_accuracy = trainStump(bcp_stump, X2_train, y2_train, X2_test, y2_test)\n",
    "ar_stump_accuracy = trainStump(ar_stump, X3_train, y3_train, X3_test, y3_test)\n",
    "\n",
    "print(\"Decision Stump Accuracy\")\n",
    "print(\"Webiste Phising:\", wp_stump_accuracy)\n",
    "print(\"BCP: \", bcp_stump_accuracy)\n",
    "print(\"Arrhythmia:\", ar_stump_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06981cc",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4e39a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy\n",
      "Webiste Phising: 0.8516508367254636\n",
      "BCP:  0.0072992700729927005\n",
      "Arrhythmia: 0.01098901098901099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#Webiste-phising\\ntree1 = DecisionTree()   \\ntree1.fit(X1_train, y1_train)\\ny1_pred = tree1.predict(X1_test)\\nacc1_tree = np.mean(y1_test == y1_pred)\\nprint(\"Accuracy webiste-phising:\", acc1_tree)\\n\\n#BCP\\ntree2 = DecisionTree()\\ntree2.fit(X2_train, y2_train)\\ny2_pred = tree2.predict(X2_test)\\nacc2_tree = np.mean(y2_test == y2_pred)\\nprint(\"Accuracy bcp:\", acc2_tree)\\n\\n#Arrhythmia\\ntree3 = DecisionTree()\\ntree3.fit(X3_train, y3_train)\\ny3_pred = tree3.predict(X3_test)\\nacc3_tree = np.mean(y3_test == y3_pred)\\nprint(\"Accuracy: arrhythmia\", acc3_tree)\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tree = DecisionTree()\n",
    "bcp_tree = DecisionTree()\n",
    "ar_tree = DecisionTree()\n",
    "\n",
    "def trainTree(tree, X_train, y_train, X_test, y_test):\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    return acc\n",
    "\n",
    "print(\"Decision Tree Accuracy\")\n",
    "wp_tree_accuracy = trainTree(wp_tree, X1_train, y1_train, X1_test, y1_test)\n",
    "print(\"Webiste Phising:\", wp_tree_accuracy)\n",
    "\n",
    "bcp_tree_accuracy = trainTree(bcp_tree, X2_train, y2_train, X2_test, y2_test)\n",
    "print(\"BCP: \", bcp_tree_accuracy)\n",
    "\n",
    "ar_tree_accuracy = trainTree(ar_tree, X3_train, y3_train, X3_test, y3_test)\n",
    "print(\"Arrhythmia:\", ar_tree_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ab36e",
   "metadata": {},
   "source": [
    "#### Pruned Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b576c818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy\n",
      "Webiste Phising: 0.8516508367254636\n",
      "BCP:  0.0072992700729927005\n",
      "Arrhythmia: 0.01098901098901099\n"
     ]
    }
   ],
   "source": [
    "wp_pruned = PrunedTree()\n",
    "bcp_pruned = PrunedTree()\n",
    "ar_pruned = PrunedTree()\n",
    "\n",
    "def trainTree(tree, X_train, y_train, X_test, y_test):\n",
    "    tree.fit(X_train, y_train, 5)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    acc = np.mean(y_test == y_pred)\n",
    "    return acc\n",
    "\n",
    "print(\"Decision Tree Accuracy\")\n",
    "wp_pruned_accuracy = trainTree(wp_pruned, X1_train, y1_train, X1_test, y1_test)\n",
    "print(\"Webiste Phising:\", wp_pruned_accuracy)\n",
    "\n",
    "bcp_pruned_accuracy = trainTree(bcp_pruned, X2_train, y2_train, X2_test, y2_test)\n",
    "print(\"BCP: \", bcp_pruned_accuracy)\n",
    "\n",
    "ar_pruned_accuracy = trainTree(ar_pruned, X3_train, y3_train, X3_test, y3_test)\n",
    "print(\"Arrhythmia:\", ar_pruned_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d446a11",
   "metadata": {},
   "source": [
    "# 3 Choosing hyperparameter\n",
    "The only hyperparameters I have used in these implementations is max_depth. I will not use a 5-fold cross-validation to evaluate the max_depth values 2, 4, 5 and 10. Evaluating the different values for the hyperparameter without the cross-validation had a running time of 119m 46.8s, and implementing a 5-fold cross-validation will make the program more complex, and therefore add time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43e9e2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Max Depth  Accuracy (Website-Phising)  Running time (Website-Phising)  \\\n",
      "0          1                    0.851651                      319.815966   \n",
      "1          2                    0.851651                      320.680798   \n",
      "2          5                    0.851651                      318.404168   \n",
      "3         10                    0.851651                      317.715161   \n",
      "\n",
      "   Accuracy (BCP)  Running time (BCP)  Accuracy (Arrhythmia)  \\\n",
      "0        0.007299           35.525393               0.010989   \n",
      "1        0.007299           34.919551               0.010989   \n",
      "2        0.007299           34.409210               0.010989   \n",
      "3        0.007299           43.015231               0.010989   \n",
      "\n",
      "   Running time (Arrhythmia)  \n",
      "0                1439.898164  \n",
      "1                1440.661396  \n",
      "2                1438.906347  \n",
      "3                1442.833914  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "max_depth_values = [2, 4, 5, 10]\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, max_depth):\n",
    "    start_time = time.time()\n",
    "    model = PrunedTree()\n",
    "    model.fit(X_train, y_train, max_depth)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = np.mean(y_test == y_pred)\n",
    "    running_time = time.time() - start_time\n",
    "    return accuracy, running_time\n",
    "\n",
    "max_depth_values = [1, 2, 5, 10]\n",
    "\n",
    "results = []\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    acc1, time1 = evaluate_model(X1_train, y1_train, X1_test, y1_test, max_depth)\n",
    "    acc2, time2 = evaluate_model(X2_train, y2_train, X2_test, y2_test, max_depth)\n",
    "    acc3, time3 = evaluate_model(X3_train, y3_train, X3_test, y3_test, max_depth)\n",
    "\n",
    "    results.append([max_depth, acc1, time1, acc2, time2, acc3, time3])\n",
    "\n",
    "columns = [\"Max Depth\", \"Accuracy (Website-Phising)\", \"Running time (Website-Phising)\",\n",
    "           \"Accuracy (BCP)\", \"Running time (BCP)\",\n",
    "           \"Accuracy (Arrhythmia)\", \"Running time (Arrhythmia)\"]\n",
    "\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147c925",
   "metadata": {},
   "source": [
    "When testing the different hyperparameters we can see that all the depths provide the same accuracy, which can point to the fact that the pruned tree might be implemented a wrong way. We can also see that the runningtime is the same, which also supports this theory. It is expected to see an increase in accuracy and in running time with the increase of max_depth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5fbec",
   "metadata": {},
   "source": [
    "# 4 Comparing models\n",
    "In this section I will compare the three methods and comment if any of the methods preforms significantly worse on each data set. To do this, I will run a p-value test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec3177b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-test for Decision stump and Decision Tree:\n",
      "  - T-statistic: -1.0440295848510426\n",
      "  - P-value: 0.40607210639704483\n",
      "  - No significant difference detected\n",
      "T-test for Decision stump and Pruned Tree:\n",
      "  - T-statistic: -1.096281081586782\n",
      "  - P-value: 0.3873356149505429\n",
      "  - No significant difference detected\n",
      "T-test for Decision Tree and Pruned Tree:\n",
      "  - T-statistic: -1.0\n",
      "  - P-value: 0.42264973081037427\n",
      "  - No significant difference detected\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Assuming you have accuracy scores for each model\n",
    "accuracy_scores = {\n",
    "    'Decision stump': [wp_stump_accuracy, bcp_stump_accuracy, ar_stump_accuracy],\n",
    "    'Decision Tree': [wp_tree_accuracy, bcp_tree_accuracy, bcp_tree_accuracy],\n",
    "    'Pruned Tree': [wp_pruned_accuracy, bcp_pruned_accuracy, ar_pruned_accuracy]\n",
    "}\n",
    "\n",
    "# Perform t-test for each pair of models\n",
    "for model1, model2 in [('Decision stump', 'Decision Tree'), ('Decision stump', 'Pruned Tree'), ('Decision Tree', 'Pruned Tree')]:\n",
    "    t_statistic, p_value = stats.ttest_rel(accuracy_scores[model1], accuracy_scores[model2])\n",
    "    print(f\"T-test for {model1} and {model2}:\")\n",
    "    print(f\"  - T-statistic: {t_statistic}\")\n",
    "    print(f\"  - P-value: {p_value}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"  - Significant difference detected\")\n",
    "    else:\n",
    "        print(\"  - No significant difference detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b61fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
